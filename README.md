# EksiGPT

Herkese merhaba! Bu projenin amacÄ± bir yapay bir EkÅŸi SÃ¶zlÃ¼k kullanÄ±cÄ±sÄ± yaratmak.

Bunu yapmak iÃ§in Generative Pre-trained Transformer (GPT) modelini hep beraber oluÅŸturacaÄŸÄ±z yani kodunu yazacaÄŸÄ±z. Bunu da YouTube'da yapacaÄŸÄ±mÄ±z bir video serisi ile birlikte gerÃ§ekleÅŸtireceÄŸiz.

Verisetimiz 700 adet ekÅŸiseyler.com'dan indirdiÄŸimiz makalelerden oluÅŸuyor (3.4 Megabyte) yani Ã§ok kÃ¼Ã§Ã¼k bir veri seti. Ama sonunda minik bir ekÅŸici oluÅŸturmamÄ±z iÃ§in yeterli olacak :)

Ä°lk videoda bir 2-gram modeli oluÅŸturduk ve tahminleri karakter seviyesinde yapÄ±yoruz. Yani bir kelimedeki sonraki karakteri tahmin ediyoruz.

UmarÄ±m beÄŸenirsiniz!

## [NÃ¶ral AÄŸlarla EkÅŸici Kodluyoruz (EkÅŸiGPT)](https://youtu.be/L7rsPZ1bGHw)

[![Watch the video](https://img.youtube.com/vi/L7rsPZ1bGHw/maxresdefault.jpg)](https://youtu.be/L7rsPZ1bGHw)

TanÄ±tÄ±m ve bigram model'in oluÅŸturulmasÄ±

Burada ilk videoyu bitirdik! Peki neler ogrendik?

- DokÃ¼manlarÄ±n iÃ§erisindeki karakter sayÄ±sÄ± boyutunu belirliyor. Bizim dokÃ¼manda 3.5 milyon karakter var yani yaklaÅŸÄ±k 3.5 Megabyte.

- Next token prediction, yani sonraki hece tahmini en Ã¶nemli fikirlerden biri. Ã‡Ã¼nkÃ¼ neyi tahmin etmemiz gerektiÄŸini artÄ±k biliyoruz.

- Normalde heceleri de tahmin edebiliriz ama biz ÅŸu anda karakterleri tahmin ediyoruz ve bunlarÄ± teker teker yapÄ±yoruz. Yani bir Ã¶nceki harf diÄŸerini takip ediyor.
  Buna literatÃ¼rde bigram model deniliyor. Matematiksel olarak da ÅŸÃ¶yle:

  ğ‘ƒ(ğ‘ğ‘–âˆ£ğ‘1,ğ‘2,â€¦,ğ‘ğ‘–âˆ’1)â‰ˆğ‘ƒ(ğ‘ğ‘–âˆ£ğ‘ğ‘–âˆ’1)

  yani, bir dÃ¶kÃ¼mandaki i sÄ±radaki karakteri tahmin etmek iÃ§in normalde ondan Ã¶nceki tÃ¼m karakterleri bilmemiz gerekir ama burada sadece i-1 inci karakteri (yani tahmin edilenden sadece bir Ã¶nceki) bilmenin tÃ¼m karakterleri bilmeye eÅŸit olduÄŸunu varsayÄ±yoruz. Tabi bu bi noktaya kadar doÄŸru ve Ã§ok da iyi bir model deÄŸil! Ã–rneÄŸin 'anen gÃ¶teÄŸ' tahmin etmesi gibi :D

## [Dikkat AlgoritmasÄ±nÄ± Kodluyoruz! (EksiGPT)](https://www.youtube.com/watch?v=u168PH3rH7A&t=818s&ab_channel=OnurKarakaslar)

[![Watch the video](https://img.youtube.com/vi/u168PH3rH7A/maxresdefault.jpg)](https://youtu.be/u168PH3rH7A)

Ä°kinci videoda dikkat (attention) mekanizmasÄ±ni kodluyoruz. BaÅŸtan baÅŸlayÄ±p attention nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlatÄ±yoruz. Daha fazla bilgi iÃ§in makaleye [buradan](https://arxiv.org/abs/1706.03762) gÃ¶z atabilirsiniz:

[Colab linki](https://colab.research.google.com/drive/1XuKz_puqsDnpLSAijQlMmd4N1tOHinMI?usp=sharing)

## Genel Bilgiler

Discord KanalÄ±mÄ±z: https://discord.gg/abCgk6hMqx
