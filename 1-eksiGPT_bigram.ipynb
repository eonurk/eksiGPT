{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTjMZbEdtoh_",
        "outputId": "1b4a1ff5-9b6e-4d90-85a4-5d00ceb04112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-04 08:10:07--  https://raw.githubusercontent.com/eonurk/eksiGPT/c03b0394808b724a89f519ba9457f0bc623bbc23/data/eksi_articles.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3647853 (3.5M) [text/plain]\n",
            "Saving to: ‘eksi_articles.txt’\n",
            "\n",
            "eksi_articles.txt   100%[===================>]   3.48M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-12-04 08:10:08 (40.7 MB/s) - ‘eksi_articles.txt’ saved [3647853/3647853]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# başlamak için verisetini indiriyoruz.\n",
        "\n",
        "!wget https://raw.githubusercontent.com/eonurk/eksiGPT/c03b0394808b724a89f519ba9457f0bc623bbc23/data/eksi_articles.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hadi veriyi inceleyelim.\n",
        "with open('eksi_articles.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "yhi7b10wuCAw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ilk 2000 karaktere bakalım\n",
        "print(text[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G9ZpYGwuU1t",
        "outputId": "558528b4-801e-41f9-b4ed-f9baa3f311b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: TARİH\n",
            "Category: TARİH\n",
            "\n",
            "Content:\n",
            "1741 yılında yedi aylık bir deniz yolculuğunun ardından hollandalı kaptan douwemout van der meer, dişi genç bir hint gergedanıyla rotterdam limanına geldi. 1579 yılından bu yana avrupa'ya gelen ilk gergedandı bu!\n",
            "\n",
            "avrupalılar için gergedan, tek boynuzlu at kadar efsanevi bir varlıktı. o güne dek gerçekten yaşadığına bir çok insanın inanmadığı hayvanın gerçekten var olduğunun tek kanıtı albrecht dürer'in 1515 tarihli gravür baskısıydı.\n",
            "\n",
            "bayan clara ismi konulan dişi hint gergedanı, 1744'te almanya'nın hamburg kentine ilk gezisini yapmadan önce birkaç yıl boyunca hollanda'da sergilendi . ardından tüm avrupayı gezen gergedan 3 kez de ingiltere'ye götürüldü ( bu seyahatler sırasında onu taşıyamadığı için çöken vagonlar, roma'da olduğu sırada kapalı tutulduğu ağırda sıkılıp boynuzunu ahşap duvarlara sürterek kırması gibi pek çok detayla sizi sıkmayalım).\n",
            "\n",
            "hayatta olduğu süreçte avrupa'da bir gergedan çılgınlığı başladı. çok sayıda gergedan temalı hediyelik eşyalar, resimler vb üretildi ve satıldı. dönemin ünlü ressamları fransız ressam jean-baptiste oudry, venedikli ressam pietro longhi gibi.\n",
            "\n",
            "isimlerin tabloları ünlü müzelerde sergilendi. bronz ve mermerden heykelleri yapılıp ünlü mekanlara konulurken, çizimleri anatomi atlaslarına girdi.\n",
            "\n",
            "aslında clara'nın hikayesi çok acıklı başlamıştı, annesi 17387'te hindistan'da öldürülünce bir hollandalı yönetici'nin evinin bahçesinde \" evcil bir hayvan olarak \" kaldı bir kaç yıl. 1740'da onu kaptan van der meer satın aldı ve ona çok iyi bakarak ülkesine götürdü. avrupa'ya getirilmeye çalışılan ya da getirilen pek çok egzotik hayvan ya yolda ya da karaya ayak bastıktan kısa süre sonra ölmüştü. diğerlerinden çok daha uzun yaşasa da clara nisan 1758'te londra'da öldü ( nedeni bilinmiyor).\n",
            "\n",
            "## Gladyatör Filminde Maliyetli ve Zor Olacağı Düşünülerek Vazgeçilen Gergedan Sahnesi\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Title: İngiltere'de Cenazelere Profesy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"karakter sayısı: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rB8fCviurOF",
        "outputId": "5c29260e-f230-4451-c533-4f6eacb7f6ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "karakter sayısı:  3359477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "karakter = sorted(set(text))\n",
        "sozluk_buyuklugu = len(karakter)\n",
        "print(\"karakter sayısı: \", sozluk_buyuklugu)\n",
        "print(\"\".join(karakter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBSuhfTuu53W",
        "outputId": "4da5ee92-f2db-470f-c144-bd6eb4cd36d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "karakter sayısı:  135\n",
            "\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz| £¯°´ÇÖ×ÜàáâäçèéêëíîñóôöúûüĞğİıŞşš–—‘’“”•…€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# karakterleri sayılara çevirelim\n",
        "ceviri = { ch:i for i,ch in enumerate(karakter) }\n",
        "irivec = { i:ch for i,ch in enumerate(karakter) }\n",
        "\n",
        "encode = lambda s: [ceviri[c] for c in s] # encoder: bi kelimeyi sayiya cevirir\n",
        "decode = lambda l: ''.join([irivec[i] for i in l]) # decode: sayiyi kelimeye cevirir\n",
        "\n",
        "\n",
        "girdi = \"onur bu video olmadi\"\n",
        "cikti = encode(girdi)\n",
        "\n",
        "print(cikti)\n",
        "print(decode(cikti))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY8rt5xjvc-b",
        "outputId": "5cafb212-ac1c-409b-f45c-1f03c4418a2b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[79, 78, 85, 82, 1, 66, 85, 1, 86, 73, 68, 69, 79, 1, 79, 76, 77, 65, 68, 73]\n",
            "onur bu video olmadi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRD8ChDdxCrQ",
        "outputId": "b76d2e5c-1818-435a-84e6-fc6ede2a92e5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3359477]) torch.int64\n",
            "tensor([ 53,  73,  84,  76,  69,  27,   1,  53,  34,  51, 121,  41,   0,  36,\n",
            "         65,  84,  69,  71,  79,  82,  89,  27,   1,  53,  34,  51, 121,  41,\n",
            "          0,   0,  36,  79,  78,  84,  69,  78,  84,  27,   0,  18,  24,  21,\n",
            "         18,   1,  89, 122,  76, 122,  78,  68,  65,   1,  89,  69,  68,  73,\n",
            "          1,  65,  89,  76, 122,  75,   1,  66,  73,  82,   1,  68,  69,  78,\n",
            "         73,  90,   1,  89,  79,  76,  67,  85,  76,  85, 120,  85,  78,  85,\n",
            "         78,   1,  65,  82,  68, 122,  78,  68,  65,  78,   1,  72,  79,  76,\n",
            "         76,  65,  78,  68,  65,  76, 122,   1,  75,  65,  80,  84,  65,  78,\n",
            "          1,  68,  79,  85,  87,  69,  77,  79,  85,  84,   1,  86,  65,  78,\n",
            "          1,  68,  69,  82,   1,  77,  69,  69,  82,  13,   1,  68,  73, 124,\n",
            "         73,   1,  71,  69,  78, 105,   1,  66,  73,  82,   1,  72,  73,  78,\n",
            "         84,   1,  71,  69,  82,  71,  69,  68,  65,  78, 122,  89,  76,  65,\n",
            "          1,  82,  79,  84,  84,  69,  82,  68,  65,  77,   1,  76,  73,  77,\n",
            "         65,  78, 122,  78,  65,   1,  71,  69,  76,  68,  73,  15,   1,  18,\n",
            "         22,  24,  26,   1,  89, 122,  76, 122,  78,  68,  65,  78,   1,  66,\n",
            "         85,   1,  89,  65,  78,  65,   1,  65,  86,  82,  85,  80,  65,   8,\n",
            "         89,  65,   1,  71,  69,  76,  69,  78,   1,  73,  76,  75,   1,  71,\n",
            "         69,  82,  71,  69,  68,  65,  78,  68, 122,   1,  66,  85,   2,   0,\n",
            "          0,  65,  86,  82,  85,  80,  65,  76, 122,  76,  65,  82,   1,  73,\n",
            "        105,  73,  78,   1,  71,  69,  82,  71,  69,  68,  65,  78,  13,   1,\n",
            "         84,  69,  75,   1,  66,  79,  89,  78,  85,  90,  76,  85,   1,  65,\n",
            "         84,   1,  75,  65,  68,  65,  82,   1,  69,  70,  83,  65,  78,  69,\n",
            "         86,  73,   1,  66,  73,  82,   1,  86,  65,  82,  76, 122,  75,  84,\n",
            "        122,  15,   1,  79,   1,  71, 118,  78,  69,   1,  68,  69,  75,   1,\n",
            "         71,  69,  82, 105,  69,  75,  84,  69,  78,   1,  89,  65, 124,  65,\n",
            "         68, 122, 120, 122,  78,  65,   1,  66,  73,  82,   1, 105,  79,  75,\n",
            "          1,  73,  78,  83,  65,  78, 122,  78,   1,  73,  78,  65,  78,  77,\n",
            "         65,  68, 122, 120, 122,   1,  72,  65,  89,  86,  65,  78, 122,  78,\n",
            "          1,  71,  69,  82, 105,  69,  75,  84,  69,  78,   1,  86,  65,  82,\n",
            "          1,  79,  76,  68,  85, 120,  85,  78,  85,  78,   1,  84,  69,  75,\n",
            "          1,  75,  65,  78, 122,  84, 122,   1,  65,  76,  66,  82,  69,  67,\n",
            "         72,  84,   1,  68, 118,  82,  69,  82,   8,  73,  78,   1,  18,  22,\n",
            "         18,  22,   1,  84,  65,  82,  73,  72,  76,  73,   1,  71,  82,  65,\n",
            "         86, 118,  82,   1,  66,  65,  83,  75, 122,  83, 122,  89,  68, 122,\n",
            "         15,   0,   0,  66,  65,  89,  65,  78,   1,  67,  76,  65,  82,  65,\n",
            "          1,  73,  83,  77,  73,   1,  75,  79,  78,  85,  76,  65,  78,   1,\n",
            "         68,  73, 124,  73,   1,  72,  73,  78,  84,   1,  71,  69,  82,  71,\n",
            "         69,  68,  65,  78, 122,  13,   1,  18,  24,  21,  21,   8,  84,  69,\n",
            "          1,  65,  76,  77,  65,  78,  89,  65,   8,  78, 122,  78,   1,  72,\n",
            "         65,  77,  66,  85,  82,  71,   1,  75,  69,  78,  84,  73,  78,  69,\n",
            "          1,  73,  76,  75,   1,  71,  69,  90,  73,  83,  73,  78,  73,   1,\n",
            "         89,  65,  80,  77,  65,  68,  65,  78,   1, 115,  78,  67,  69,   1,\n",
            "         66,  73,  82,  75,  65, 105,   1,  89, 122,  76,   1,  66,  79,  89,\n",
            "         85,  78,  67,  65,   1,  72,  79,  76,  76,  65,  78,  68,  65,   8,\n",
            "         68,  65,   1,  83,  69,  82,  71,  73,  76,  69,  78,  68,  73,   1,\n",
            "         15,   1,  65,  82,  68, 122,  78,  68,  65,  78,   1,  84, 118,  77,\n",
            "          1,  65,  86,  82,  85,  80,  65,  89, 122,   1,  71,  69,  90,  69,\n",
            "         78,   1,  71,  69,  82,  71,  69,  68,  65,  78,   1,  20,   1,  75,\n",
            "         69,  90,   1,  68,  69,   1,  73,  78,  71,  73,  76,  84,  69,  82,\n",
            "         69,   8,  89,  69,   1,  71, 115,  84, 118,  82, 118,  76,  68, 118,\n",
            "          1,   9,   1,  66,  85,   1,  83,  69,  89,  65,  72,  65,  84,  76,\n",
            "         69,  82,   1,  83, 122,  82,  65,  83, 122,  78,  68,  65,   1,  79,\n",
            "         78,  85,   1,  84,  65, 124, 122,  89,  65,  77,  65,  68, 122, 120,\n",
            "        122,   1,  73, 105,  73,  78,   1, 105, 115,  75,  69,  78,   1,  86,\n",
            "         65,  71,  79,  78,  76,  65,  82,  13,   1,  82,  79,  77,  65,   8,\n",
            "         68,  65,   1,  79,  76,  68,  85, 120,  85,   1,  83, 122,  82,  65,\n",
            "         68,  65,   1,  75,  65,  80,  65,  76, 122,   1,  84,  85,  84,  85,\n",
            "         76,  68,  85, 120,  85,   1,  65, 120, 122,  82,  68,  65,   1,  83,\n",
            "        122,  75, 122,  76, 122,  80,   1,  66,  79,  89,  78,  85,  90,  85,\n",
            "         78,  85,   1,  65,  72, 124,  65,  80,   1,  68,  85,  86,  65,  82,\n",
            "         76,  65,  82,  65,   1,  83, 118,  82,  84,  69,  82,  69,  75,   1,\n",
            "         75, 122,  82,  77,  65,  83, 122,   1,  71,  73,  66,  73,   1,  80,\n",
            "         69,  75,   1, 105,  79,  75,   1,  68,  69,  84,  65,  89,  76,  65,\n",
            "          1,  83,  73,  90,  73,   1,  83, 122,  75,  77,  65,  89,  65,  76,\n",
            "        122,  77,  10,  15,   0,   0,  72,  65,  89,  65,  84,  84,  65,   1,\n",
            "         79,  76,  68,  85, 120,  85,   1,  83, 118,  82,  69, 105,  84,  69,\n",
            "          1,  65,  86,  82,  85,  80,  65,   8,  68,  65,   1,  66,  73,  82,\n",
            "          1,  71,  69,  82,  71,  69,  68,  65,  78,   1, 105, 122,  76,  71,\n",
            "        122,  78,  76, 122, 120, 122,   1,  66,  65, 124,  76,  65,  68, 122,\n",
            "         15,   1, 105,  79,  75,   1,  83,  65,  89, 122,  68,  65,   1,  71,\n",
            "         69,  82,  71,  69,  68,  65,  78,   1,  84,  69,  77,  65,  76, 122,\n",
            "          1,  72,  69,  68,  73,  89])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "print(n)\n",
        "\n",
        "egitim = data[:n]\n",
        "test = data[n:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBnUZjIDyu3_",
        "outputId": "1c05eabf-8759-425e-a33f-a53957bc0709"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3023529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pencere = 8\n",
        "\n",
        "decode(egitim[:pencere].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sn0pZFgbzZ0H",
        "outputId": "7047dc95-4e6c-47d3-b145-aecfce120e46"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Title: T'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# next token prediction : sonraki heceyi tahmin\n",
        "\n",
        "x = egitim[:pencere]\n",
        "y = egitim[1:pencere+1]\n",
        "\n",
        "for t in range(pencere):\n",
        "    girdi = x[:t+1]\n",
        "    cikti = y[t]\n",
        "    print(f\"girdi: {girdi} cikti: {cikti}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGrgDO9dz7CM",
        "outputId": "2e7db0db-16ce-49a8-894e-bb034378e1aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "girdi: tensor([53]) cikti: 73\n",
            "girdi: tensor([53, 73]) cikti: 84\n",
            "girdi: tensor([53, 73, 84]) cikti: 76\n",
            "girdi: tensor([53, 73, 84, 76]) cikti: 69\n",
            "girdi: tensor([53, 73, 84, 76, 69]) cikti: 27\n",
            "girdi: tensor([53, 73, 84, 76, 69, 27]) cikti: 1\n",
            "girdi: tensor([53, 73, 84, 76, 69, 27,  1]) cikti: 53\n",
            "girdi: tensor([53, 73, 84, 76, 69, 27,  1, 53]) cikti: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "irivec[30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f2Yx9EJr3aAL",
        "outputId": "6a1f7c53-40c1-46e5-9fbe-e78da12d1cdc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'='"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randint(len(data) - pencere, (135,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkg7sMc2Eo92",
        "outputId": "25c8c0cc-9c1b-4a03-bd5f-5361f2124e00"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3214428, 2261923, 2916989, 2648489, 1515006, 2496588, 1897310, 2360446,\n",
              "        1957980, 2212464, 3084998, 2117740,  908837, 2226090, 3119378, 1208542,\n",
              "         122816, 2242072,  997824,  900568,  552365, 1649054, 1027382,  718861,\n",
              "        3294556, 2664511, 2356029, 1221639, 2863303,  970164, 1645838,  693960,\n",
              "         163146,  522755, 2783753, 1109376,  324824,  431674, 1061113, 1748629,\n",
              "        3289727,  363804,  728752, 3241168, 1848855,  823231, 2159091, 2121684,\n",
              "        2689751, 3291177, 2596121,  563877, 2439666, 2228511, 2364621, 3047075,\n",
              "        1860791, 1844327, 1422337, 1226523,  318987,  702426, 2587319, 3305310,\n",
              "        3201177,  843756, 1573154, 1115801, 2215372,  631584,  250812,   52344,\n",
              "        1322408, 1328144, 2743445, 2591878, 3072138, 1386819,  386053, 2014961,\n",
              "        3137669, 1446794, 1254294, 2663590, 1811119, 2647290, 1042172,  587076,\n",
              "        2564797, 3251997,  922660, 2496522,  843381, 1403777, 1457655,  206804,\n",
              "        2152099, 2416365, 2456420, 1786416, 2116291,   74885, 1719595,  564579,\n",
              "        2754280,  739254,  447925, 1652069, 1836127, 2651279,  399294, 1101807,\n",
              "        3078103, 2426479, 1638229, 2137157, 2841161, 2949974, 2070447, 1936939,\n",
              "        3255829,  905682, 1508534,  718922, 2139649,  153431, 1806494, 2674621,\n",
              "        2885856,  303758, 1457164, 1435345,  401617, 1592044, 2174910])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kume = 4 # batch (B)\n",
        "pencere = 8 # context window (T)\n",
        "\n",
        "\n",
        "def get_batch(split, kume = 4):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = egitim if split == 'egitim' else test\n",
        "    ix = torch.randint(len(data) - pencere, (kume,))\n",
        "    x = torch.stack([data[i:i+pencere] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+pencere+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('egitim')\n",
        "print('girdi:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('cikti:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(kume):\n",
        "    for t in range(pencere):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"girdi {context.tolist()} cikti: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksLZ7j131B7u",
        "outputId": "ac2768b3-c502-4ff5-fb40-b54773c50648"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "girdi:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 77,  69,  89,  69,   1,  71,  69,  82],\n",
            "        [ 78,  65,  83,  82,  65,  76,  76,  65],\n",
            "        [  1,  65,  82,  65,  66,  65,  76,  65],\n",
            "        [ 20,  15,   1,  77, 115,   1,  21,  23]])\n",
            "cikti:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 69,  89,  69,   1,  71,  69,  82,  69],\n",
            "        [ 65,  83,  82,  65,  76,  76,  65,  72],\n",
            "        [ 65,  82,  65,  66,  65,  76,  65,  82],\n",
            "        [ 15,   1,  77, 115,   1,  21,  23,   1]])\n",
            "----\n",
            "girdi [77] cikti: 69\n",
            "girdi [77, 69] cikti: 89\n",
            "girdi [77, 69, 89] cikti: 69\n",
            "girdi [77, 69, 89, 69] cikti: 1\n",
            "girdi [77, 69, 89, 69, 1] cikti: 71\n",
            "girdi [77, 69, 89, 69, 1, 71] cikti: 69\n",
            "girdi [77, 69, 89, 69, 1, 71, 69] cikti: 82\n",
            "girdi [77, 69, 89, 69, 1, 71, 69, 82] cikti: 69\n",
            "girdi [78] cikti: 65\n",
            "girdi [78, 65] cikti: 83\n",
            "girdi [78, 65, 83] cikti: 82\n",
            "girdi [78, 65, 83, 82] cikti: 65\n",
            "girdi [78, 65, 83, 82, 65] cikti: 76\n",
            "girdi [78, 65, 83, 82, 65, 76] cikti: 76\n",
            "girdi [78, 65, 83, 82, 65, 76, 76] cikti: 65\n",
            "girdi [78, 65, 83, 82, 65, 76, 76, 65] cikti: 72\n",
            "girdi [1] cikti: 65\n",
            "girdi [1, 65] cikti: 82\n",
            "girdi [1, 65, 82] cikti: 65\n",
            "girdi [1, 65, 82, 65] cikti: 66\n",
            "girdi [1, 65, 82, 65, 66] cikti: 65\n",
            "girdi [1, 65, 82, 65, 66, 65] cikti: 76\n",
            "girdi [1, 65, 82, 65, 66, 65, 76] cikti: 65\n",
            "girdi [1, 65, 82, 65, 66, 65, 76, 65] cikti: 82\n",
            "girdi [20] cikti: 15\n",
            "girdi [20, 15] cikti: 1\n",
            "girdi [20, 15, 1] cikti: 77\n",
            "girdi [20, 15, 1, 77] cikti: 115\n",
            "girdi [20, 15, 1, 77, 115] cikti: 1\n",
            "girdi [20, 15, 1, 77, 115, 1] cikti: 21\n",
            "girdi [20, 15, 1, 77, 115, 1, 21] cikti: 23\n",
            "girdi [20, 15, 1, 77, 115, 1, 21, 23] cikti: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1500)\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # basit bir tablo (icerisinde olasiliklar var)\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, hedef=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if hedef is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            hedef = hedef.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, hedef)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def tahmin(self, idx, max_karakter):\n",
        "\n",
        "        for _ in range(max_karakter):\n",
        "\n",
        "            logits, loss = self(idx)\n",
        "\n",
        "            # sonuncu zamana odaklan\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # olasiliklari her bir kume icin normalize ediyoruz\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # yeni bir karakter seciyoruz\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # sona ekle\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(sozluk_buyuklugu)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbqiro2T3lEB",
        "outputId": "582ad358-9f1a-4e94-c108-316325b5fff0"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 135])\n",
            "tensor(5.2559, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ornekleme\n",
        "torch.multinomial(torch.tensor([0.1,0.1, 0.1, 0.1, 0.8]), num_samples=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DEdPApkSQp1",
        "outputId": "c9da7604-0f06-450f-e4e1-e15097134b03"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4])"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.tahmin(idx = torch.zeros((1, 1), dtype=torch.long), max_karakter=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWStkl3uUkIH",
        "outputId": "7b4ef301-6219-4223-c582-40b7bf26394a"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HYz.qcs3éê’Ml m[ +Zé2]ücEYrîóYê£e“,°$J|^ı'šFa–Çsc=#èufêâ'¯bZTxA/d×ûBcEMjó•OñTLşRq=U,THÇ[AWRG¯zLë…ó#t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer objesi olusturduk\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "H1C7cXa5XIq7"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kume = 64\n",
        "for adim in range(1000):\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('egitim', kume=kume)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TscRvcrtXQNF",
        "outputId": "d1820a7d-fa2c-453b-9e7a-dca3ae3aab5c"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5028345584869385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.ones((1, 1), dtype=torch.long)\n",
        "\n",
        "print(decode(m.tahmin(idx, max_karakter=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIzFRrHFY8Mk",
        "outputId": "a21966b9-9e1a-4899-cc15-e24ec4c4c9b5"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " |-leni dedasüşı i ezır ön argöyrndeG”ızam, vi lktı ertüverstise kabu201.\n",
            "\n",
            "Caywmardoldene dakemı bins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8000000000//(135*135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rQo5KLZbWoW",
        "outputId": "66de62a5-bfb1-4a2e-e602-1824f5058028"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "438957"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/eonurk/eksiGPT"
      ],
      "metadata": {
        "id": "g05YQG_5aeF7"
      }
    }
  ]
}